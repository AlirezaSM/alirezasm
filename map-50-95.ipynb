{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69cc7a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T16:17:55.228323Z",
     "iopub.status.busy": "2024-10-23T16:17:55.227841Z",
     "iopub.status.idle": "2024-10-23T16:17:56.230212Z",
     "shell.execute_reply": "2024-10-23T16:17:56.229036Z"
    },
    "papermill": {
     "duration": 1.009699,
     "end_time": "2024-10-23T16:17:56.233223",
     "exception": false,
     "start_time": "2024-10-23T16:17:55.223524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "class_names = {0: 'aeroplane',\n",
    "               1: 'bicycle',\n",
    "               2: 'bird',\n",
    "               3: 'boat',\n",
    "               4: 'bottle',\n",
    "               5: 'bus',\n",
    "               6: 'car',\n",
    "               7: 'cat',\n",
    "               8: 'chair',\n",
    "               9: 'cow',\n",
    "               10: 'diningtable',\n",
    "               11: 'dog',\n",
    "               12: 'horse',\n",
    "               13: 'motorbike',\n",
    "               14: 'person',\n",
    "               15: 'pottedplant',\n",
    "               16: 'sheep',\n",
    "               17: 'sofa',\n",
    "               18: 'train',\n",
    "               19: 'tvmonitor'}\n",
    "\n",
    "# Function to convert center-based bounding boxes to COCO format (x_min, y_min, width, height)\n",
    "def convert_bbox(x_center, y_center, width, height, img_width, img_height):\n",
    "    x_min = (x_center - width / 2) * img_width\n",
    "    y_min = (y_center - height / 2) * img_height\n",
    "    return [x_min, y_min, width * img_width, height * img_height]\n",
    "\n",
    "# Function to convert ground truth DataFrame to COCO ground truth format\n",
    "def df_to_coco_ground_truth(df):\n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "    \n",
    "    image_ids = []\n",
    "    annotations = []\n",
    "    categories = []\n",
    "    \n",
    "    annotation_id = 1\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = row['image_id']\n",
    "        class_id = int(row['class_id'])\n",
    "        x_center = float(row['x_center'])\n",
    "        y_center = float(row['y_center'])\n",
    "        width = float(row['width'])\n",
    "        height = float(row['height'])\n",
    "        img_width = float(row['image_width'])   # Get the specific width for this image\n",
    "        img_height = float(row['image_height']) # Get the specific height for this image\n",
    "        \n",
    "        # Add image information (assuming unique image_ids)\n",
    "        if image_id not in image_ids:\n",
    "            image_ids.append(image_id)\n",
    "            coco_output['images'].append({\n",
    "                \"id\": int(image_id.split('.')[0]),\n",
    "                \"file_name\": image_id,  # Adjust the extension if necessary\n",
    "                \"width\": img_width,\n",
    "                \"height\": img_height\n",
    "            })\n",
    "        \n",
    "        # Add category information if class_id not seen before\n",
    "        if class_id not in categories:\n",
    "            categories.append(class_id)\n",
    "            coco_output['categories'].append({\n",
    "                \"id\": class_id,\n",
    "                \"name\": class_names[class_id]  # Assign a default name like \"class_x\"\n",
    "            })\n",
    "        \n",
    "        # Convert bbox from center format to COCO format\n",
    "        bbox = convert_bbox(x_center, y_center, width, height, img_width, img_height)\n",
    "        \n",
    "        # Add annotation information\n",
    "        annotations.append({\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": int(image_id.split('.')[0]),\n",
    "            \"category_id\": class_id,\n",
    "            \"bbox\": bbox,\n",
    "            \"area\": bbox[2] * bbox[3],  # area = width * height\n",
    "            \"iscrowd\": 0  # Assuming no crowd annotations\n",
    "        })\n",
    "        annotation_id += 1\n",
    "    \n",
    "    coco_output['annotations'] = annotations\n",
    "    return coco_output\n",
    "\n",
    "# Function to convert predictions DataFrame to COCO predictions format\n",
    "def df_to_coco_predictions(df):\n",
    "    predictions = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = row['image_id']\n",
    "        class_id = int(row['class_id'])\n",
    "        confidence = float(row['confidence'])\n",
    "        x_center = float(row['x_center'])\n",
    "        y_center = float(row['y_center'])\n",
    "        width = float(row['width'])\n",
    "        height = float(row['height'])\n",
    "        img_width = float(row['image_width'])   # Get the specific width for this image\n",
    "        img_height = float(row['image_height']) # Get the specific height for this image\n",
    "        \n",
    "        # Convert bbox from center format to COCO format\n",
    "        bbox = convert_bbox(x_center, y_center, width, height, img_width, img_height)\n",
    "        \n",
    "        # Add prediction entry\n",
    "        predictions.append({\n",
    "            \"image_id\": int(image_id.split('.')[0]),\n",
    "            \"category_id\": class_id,\n",
    "            \"bbox\": bbox,\n",
    "            \"score\": confidence\n",
    "        })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Function to write the output COCO JSON files\n",
    "def write_coco_json(output_data, output_path):\n",
    "    with open(output_path, 'w') as json_file:\n",
    "        json.dump(output_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255b1978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-23T16:17:56.241132Z",
     "iopub.status.busy": "2024-10-23T16:17:56.239548Z",
     "iopub.status.idle": "2024-10-23T16:17:56.249526Z",
     "shell.execute_reply": "2024-10-23T16:17:56.248322Z"
    },
    "papermill": {
     "duration": 0.01652,
     "end_time": "2024-10-23T16:17:56.252211",
     "exception": false,
     "start_time": "2024-10-23T16:17:56.235691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the mean Average Precision (mAP) at IoU threshold 0.50 (mAP@50) for object detection predictions.\n",
    "\n",
    "    This function evaluates the quality of predicted bounding boxes against the ground truth using the COCO evaluation metric.\n",
    "    It converts both the ground truth and predictions from the provided DataFrames to COCO format, then uses the COCO evaluation \n",
    "    toolkit to compute the mAP@50.\n",
    "\n",
    "    Args:\n",
    "        solution (pd.DataFrame): Ground truth DataFrame containing the bounding boxes and labels.\n",
    "        submission (pd.DataFrame): Predicted bounding boxes and labels in a DataFrame format.\n",
    "        row_id_column_name (str): The name of the column in both DataFrames that contains the unique image identifiers.\n",
    "\n",
    "    Returns:\n",
    "        float: The mAP@IoU=0.50 score, which represents the quality of the predictions with respect to the ground truth.\n",
    "    \"\"\"\n",
    "    \n",
    "    ground_truth_json = 'groundtruth_coco.json'\n",
    "    predicted_json = 'predictions_coco.json'\n",
    "    \n",
    "    # Convert ground truth DataFrame to COCO format\n",
    "    coco_gt_data = df_to_coco_ground_truth(solution)\n",
    "    write_coco_json(coco_gt_data, ground_truth_json)\n",
    "    \n",
    "    # Convert predictions DataFrame to COCO format\n",
    "    coco_predictions_data = df_to_coco_predictions(submission)\n",
    "    write_coco_json(coco_predictions_data, predicted_json)\n",
    "    \n",
    "    # Load Ground Truth Annotations\n",
    "    coco_gt = COCO(ground_truth_json)\n",
    "    \n",
    "    # Load Predicted Annotations\n",
    "    coco_dt = coco_gt.loadRes(predicted_json)\n",
    "    \n",
    "    # Initialize COCOeval\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, 'bbox')\n",
    "    \n",
    "    # Set IoU threshold to 0.5 for mAP50\n",
    "    # coco_eval.params.iouThrs = [0.5]\n",
    "    # print(coco_eval.params.imgIds)\n",
    "    \n",
    "    # Evaluate the detections\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    # coco_eval.summarize()\n",
    "    \n",
    "    # Get the specific mAP@IoU=0.50 score\n",
    "    mAP_50 = coco_eval.stats[0]  # This corresponds to the mAP@0.50\n",
    "    return mAP50"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.457695,
   "end_time": "2024-10-23T16:17:56.776608",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-23T16:17:52.318913",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
